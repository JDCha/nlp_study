# Deep Learning with NLP 101
**자연어 처리 입문부터 최신 흐름을 이해하기 까지, 논문과 튜토리얼을 통해 달려보는 스터디 !** 

### 선수지식
- 파이썬 프로그래밍 기본 문법
- 딥러닝 기본 지식: **'모두를 위한 딥러닝'** 강의를 완강하신 경험만 있으면 충분합니다 !
- 스터디 기간 중 **최소 한 번** 이상 발제를 할 용기

<br/>

### 함께하는 사람
[**허 훈**](https://github.com/Huffon), [**이인환**](https://github.com/lih0905), 남혜리, 박병준, [**정민수**](https://github.com/4seaday), 김진원, 정지용, [**허무지**](https://github.com/Moo-Ji), [**이명학**](https://github.com/myeonghak), 전다해, [**정민소**](https://github.com/minssoj), [**임송현**](https://github.com/shyun46)

<br/>

### 스터디 난이도
🌝🌝🌗🌚🌚

<br/>

### 배우게 될 것
1. **Theory**: 딥러닝을 이용한 자연어 처리의 전반적 흐름을 논문과 아티클을 통해 학습합니다.
2. **Hands-on**: 아직 한글을 지원하지는 않지만, 최근 텍스트 전처리에 널리 사용되는 `spaCy` 라이브러리를 튜토리얼을 통해 학습합니다.
3. **Hands-on**: `PyTorch` Tutorial이 제공하는 자연어 처리 실습 코드를 돌려보며, 실제 자연어 처리 프로그램이 어떤 방식으로 학습되는지에 대해 학습합니다.

<br/>

## Month 1
딥러닝을 이용한 자연어 처리의 발전 과정과 이에 사용되는 기본적인 모델에 대해 알아봅니다.

---

- Week 1: [Recent Trends in Deep Learning Based Natural Language Processing](https://arxiv.org/pdf/1708.02709.pdf)
- Week 2: [Recurrent Neural Network](https://ko.coursera.org/lecture/nlp-sequence-models/recurrent-neural-network-model-ftkzt) + spaCy Chapter 1: Finding words, phrases, names and concepts ( 명학 / 지용 )
- Week 3: [Long Short-Term Memory](https://colah.github.io/posts/2015-08-Understanding-LSTMs/) + spaCy Chapter 2: Large-scale data analysis with spaCy ( 인환 / 훈 )
- Week 4: [Gated Recurrent Unit](https://arxiv.org/pdf/1412.3555.pdf) + spaCy Chapter 3: Processing Pipelines ( 무지 / 훈 )

<br/>

## Month 2
컴퓨터에게 인간의 언어를 이해시키기 위해 필요한 단어 임베딩 기법을 발전순으로 알아봅니다.

---

- Week 5: [Word2Vec](https://papers.nips.cc/paper/5021-distributed-representations-of-words-and-phrases-and-their-compositionality.pdf) + spaCy Chapter 4: Training a neural network model ( 병준 / 민소 )
- Week 6: [Glove](https://nlp.stanford.edu/pubs/glove.pdf) + DEEP LEARNING WITH PYTORCH: A 60 MINUTE BLITZ ( 민수 / 명학 )
- Week 7: [Fasttext](https://arxiv.org/pdf/1607.01759.pdf) + [ELMo](https://arxiv.org/pdf/1802.05365.pdf) ( 혜리 / 무지 )
- Week 8: [Word-Piece Model](https://arxiv.org/pdf/1609.08144.pdf) + DEEP LEARNING FOR NLP WITH PYTORCH #1 ( 지용 / 진원 )

<br/>

## Month 3
수 많은 자연어 처리 Task 중 기계번역 Task의 발전사를 논문을 통해 알아봅니다.

---

- Week 9: [Sequence-to-Sequence](https://papers.nips.cc/paper/5346-sequence-to-sequence-learning-with-neural-networks.pdf) [(PyTorch Implementation)](https://github.com/Huffon/pytorch-seq2seq-kor-eng) + DEEP LEARNING FOR NLP WITH PYTORCH #2 ( 훈 / 인환 )
- Week 10: [Attention](https://arxiv.org/pdf/1409.0473.pdf) + What is torch.nn really? ( 송현 / 훈 )
- Week 11: [Transformer](https://papers.nips.cc/paper/7181-attention-is-all-you-need.pdf) + Annotated Transformer ( 인환 /  )
- Week 12: [Natural Language Processing with. CNN](https://arxiv.org/pdf/1408.5882.pdf) + Closing party🎉 ( 다해 / all )
