{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 4. Training a neural network model\n",
    "\n",
    "*  how to update spaCy's statistical models to customize them for your use case\n",
    "    * to predict a new entity type in online comments\n",
    "* write your own training loop from scratch, and understand the basics of how training works, along with tips and tricks that can make your custom NLP projects more successful"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 01. Training and updating models\n",
    "\n",
    "### Why updating the model?\n",
    "* Better results on your __specific domain__\n",
    "* Learn __classification schemes__ specifically for your problem\n",
    "* Essential for __text classification__\n",
    "* Very useful for __named entity recognition__ (개체명 인식)\n",
    "* Less critical for __part-of-speech tagging__ and __dependency parsing__\n",
    "\n",
    "### How training works (1)\n",
    "1. __Initialize__ the model weights randomly with `nlp.begin_training`\n",
    "2. __Predict__ a few examples with the current weights by calling `nlp.update`\n",
    "3. __Compare__ prediction with true labels\n",
    "4. __Calculate__ how to change weights to improve predictions\n",
    "5. __Update__ weights slightly\n",
    "6. Go back to __2__.\n",
    "\n",
    "### How training works (2)\n",
    "![](https://course.spacy.io/training.png)\n",
    "* __Training data__: Examples and their annotations.\n",
    "* __Text__: The input text the model should predict a label for.\n",
    "* __Label__: The label the model should predict.\n",
    "* __Gradient__: How to change the weights.\n",
    "\n",
    "### Example: Training the __entity recognizer__\n",
    "* The entity recognizer tags words and phrases in context\n",
    "* Each token can only be part of one entity\n",
    "* Examples need to come with context\n",
    "`(\"iPhone X is coming\", {'entities': [(0, 8, 'GADGET')]})`\n",
    "* Texts with no entities are also important\n",
    "`(\"I need a new phone! Any tips?\", {'entities': []})`\n",
    "* __Goal__: teach the model to generalize\n",
    "\n",
    "### The training data\n",
    "* Examples of what we want the model to predict in context\n",
    "* Update an __existing model__: a few hundred to a few thousand examples\n",
    "* Train a __new category__: a few thousand to a million examples\n",
    "    * spaCy's English models: 2 million words\n",
    "* Usually created manually by human annotators\n",
    "* Can be semi-automated – for example, using spaCy's `Matcher`!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 02. Purpose of training\n",
    "While spaCy comes with a range of pre-trained models to predict linguistic annotations, you almost always want to fine-tune them with more examples. You can do this by training them with more labelled data.\n",
    "\n",
    "### What does training not help with?\n",
    "* Improve model accuracy on your data.\n",
    "* Learn new classification schemes.\n",
    "* Discover patterns in unlabelled data. (x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 03. Creating training data(1)\n",
    "spaCy’s __rule-based Matcher__ is a great way to quickly __create training data__ for named entity models. A list of sentences is available as the variable __TEXTS__. You can print it the IPython shell to inspect it. We want to find all mentions of different iPhone models, so we can create training data to teach a model to recognize them as __'GADGET'__.\n",
    "\n",
    "* Write a pattern for two tokens whose lowercase forms match '`iphone'` and `'x'`.\n",
    "* Write a pattern for two tokens: one token whose lowercase form matches `'iphone'` and an optional digit using the `'?'` operator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from spacy.matcher import Matcher\n",
    "from spacy.lang.en import English\n",
    "\n",
    "with open(\"exercises/iphone.json\") as f:\n",
    "    TEXTS = json.loads(f.read())\n",
    "\n",
    "nlp = English()\n",
    "matcher = Matcher(nlp.vocab)\n",
    "\n",
    "# Two tokens whose lowercase forms match 'iphone' and 'x'\n",
    "pattern1 = [{\"LOWER\": \"iphone\"}, {\"LOWER\": \"x\"}]\n",
    "\n",
    "# Token whose lowercase form matches 'iphone' and an optional digit\n",
    "pattern2 = [{\"LOWER\": \"iphone\"}, {\"IS_DIGIT\": True, \"OP\": \"?\"}]\n",
    "\n",
    "# Add patterns to the matcher\n",
    "matcher.add(\"GADGET\", None, pattern1, pattern2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 04. Creating training data(2)\n",
    "Let’s use the match patterns we’ve created in the previous exercise to bootstrap a set of training examples. A list of sentences is available as the variable `TEXTS`.\n",
    "\n",
    "* Create a doc object for each text using `nlp.pipe`.\n",
    "* Match on the `doc` and create a list of matched spans.\n",
    "* Get `(start character, end character, label)` __tuples__ of matched spans.\n",
    "* Format each example as a tuple of the text and a dict, mapping `'entities'` to the entity tuples.\n",
    "* Append the example to `TRAINING_DATA` and inspect the printed data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from spacy.matcher import Matcher\n",
    "from spacy.lang.en import English\n",
    "\n",
    "with open(\"exercises/iphone.json\") as f:\n",
    "    TEXTS = json.loads(f.read())\n",
    "\n",
    "nlp = English()\n",
    "matcher = Matcher(nlp.vocab)\n",
    "pattern1 = [{\"LOWER\": \"iphone\"}, {\"LOWER\": \"x\"}]\n",
    "pattern2 = [{\"LOWER\": \"iphone\"}, {\"IS_DIGIT\": True, \"OP\": \"?\"}]\n",
    "matcher.add(\"GADGET\", None, pattern1, pattern2)\n",
    "\n",
    "TRAINING_DATA = []\n",
    "\n",
    "# Create a Doc object for each text in TEXTS\n",
    "for doc in nlp.pipe(TEXTS):\n",
    "    # Match on the doc and create a list of matched spans\n",
    "    spans = [doc[start:end] for match_id, start, end in matcher(doc)]\n",
    "    # Get (start character, end character, label) tuples of matches\n",
    "    entities = [(span.start_char, span.end_char, \"GADGET\") for span in spans]\n",
    "    # Format the matches as a (doc.text, entities) tuple\n",
    "    training_example = (doc.text, {\"entities\": entities})\n",
    "    # Append the example to the training data\n",
    "    TRAINING_DATA.append(training_example)\n",
    "\n",
    "print(*TRAINING_DATA, sep=\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "('How to preorder the iPhone X', {'entities': [(20, 28, 'GADGET'), (20, 26, 'GADGET')]})\n",
    "\n",
    "('iPhone X is coming', {'entities': [(0, 8, 'GADGET'), (0, 6, 'GADGET')]})\n",
    "\n",
    "('Should I pay $1,000 for the iPhone X?', {'entities': [(28, 36, 'GADGET'), (28, 34, 'GADGET')]})\n",
    "\n",
    "('The iPhone 8 reviews are here', {'entities': [(4, 12, 'GADGET')]})\n",
    "\n",
    "('Your iPhone goes up to 11 today', {'entities': [(5, 11, 'GADGET')]})\n",
    "\n",
    "('I need a new phone! Any tips?', {'entities': []})"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 05. The training loop\n",
    "\n",
    "### The steps of a training loop\n",
    "1. __Loop__ for a number of times.\n",
    "2. __Shuffle__ the training data.\n",
    "3. __Divide__ the data into batches.\n",
    "4. __Update__ the model for each batch.\n",
    "5. __Save__ the updated model.\n",
    "\n",
    "### Recap: How training works\n",
    "![](https://course.spacy.io/training.png)\n",
    "* __Training data__: Examples and their annotations.\n",
    "* __Text__: The input text the model should predict a label for.\n",
    "* __Label__: The label the model should predict.\n",
    "* __Gradient__: How to change the weights.\n",
    "\n",
    "### Example loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'path_to_model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-4-f8bf4fdeeb8c>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     22\u001b[0m \u001b[1;31m# Save the model\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 23\u001b[1;33m \u001b[0mnlp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_disk\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath_to_model\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'path_to_model' is not defined"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import spacy\n",
    "from spacy.lang.en import English\n",
    "nlp = English()\n",
    "\n",
    "TRAINING_DATA = [\n",
    "    (\"How to preorder the iPhone X\", {'entities': [(20, 28, 'GADGET')]})\n",
    "    # And many more examples...\n",
    "]\n",
    "# Loop for 10 iterations\n",
    "for i in range(10):\n",
    "    # Shuffle the training data\n",
    "    random.shuffle(TRAINING_DATA)\n",
    "    # Create batches and iterate over them\n",
    "    for batch in spacy.util.minibatch(TRAINING_DATA):\n",
    "        # Split the batch in texts and annotations\n",
    "        texts = [text for text, annotation in batch]\n",
    "        annotations = [annotation for text, annotation in batch]\n",
    "        # Update the model\n",
    "        nlp.update(texts, annotations)\n",
    "\n",
    "# Save the model\n",
    "nlp.to_disk(path_to_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Updating an existing model\n",
    "* Improve the predictions on new data\n",
    "* Especially useful to improve existing categories, like __`PERSON`__ or __`ORGANIZATION`__\n",
    "* Also possible to add new categories\n",
    "* Be careful and make sure the model doesn't \"forget\" the old ones\n",
    "\n",
    "### Setting up a new pipeline from scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start with blank English model\n",
    "nlp = spacy.blank('en')\n",
    "# Create blank entity recognizer and add it to the pipeline\n",
    "ner = nlp.create_pipe('ner')\n",
    "nlp.add_pipe(ner)\n",
    "# Add a new label\n",
    "ner.add_label('GADGET')\n",
    "\n",
    "# Start the training\n",
    "nlp.begin_training()\n",
    "# Train for 10 iterations\n",
    "for itn in range(10):\n",
    "    random.shuffle(examples)\n",
    "    # Divide examples into batches\n",
    "    for batch in spacy.util.minibatch(examples, size=2):\n",
    "        texts = [text for text, annotation in batch]\n",
    "        annotations = [annotation for text, annotation in batch]\n",
    "        # Update the model\n",
    "        nlp.update(texts, annotations)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 06. Setting up the pipeline\n",
    "\n",
    "In this exercise, you’ll prepare a spaCy pipeline to train the entity recognizer to recognize `'GADGET'` entities in a text – for example, “iPhone X”.\n",
    "\n",
    "* Create a blank `'en'` model, for example using the `spacy.blank` method.\n",
    "* Create a new entity recognizer using `nlp.create_pipe` and add it to the pipeline.\n",
    "* Add the new label `'GADGET'` to the entity recognizer using the `add_label` method on the pipeline component."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "# Create a blank 'en' model\n",
    "nlp = spacy.blank(\"en\")\n",
    "\n",
    "# Create a new entity recognizer and add it to the pipeline\n",
    "ner = nlp.create_pipe(\"ner\")\n",
    "nlp.add_pipe(ner)\n",
    "\n",
    "# Add the label 'GADGET' to the entity recognizer\n",
    "ner.add_label(\"GADGET\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 07. Building a training loop\n",
    "Let’s write a simple training loop from scratch!\n",
    "\n",
    "The pipeline you’ve created in the previous exercise is available as the `nlp` object. It already contains the entity recognizer with the added label `'GADGET'`.\n",
    "\n",
    "The small set of labelled examples that you’ve created previously is available as `TRAINING_DATA`. To see the examples, you can print them in your script.\n",
    "\n",
    "* Call `nlp.begin_training`, create a training loop for 10 iterations and shuffle the training data.\n",
    "* Create batches of training data using `spacy.util.minibatch` and iterate over the batches.\n",
    "* Convert the `(text, annotations)` tuples in the batch to lists of `texts` and `annotations`.\n",
    "* For each batch, use `nlp.update` to update the model with the texts and annotations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "import random\n",
    "import json\n",
    "\n",
    "with open(\"exercises/gadgets.json\") as f:\n",
    "    TRAINING_DATA = json.loads(f.read())\n",
    "\n",
    "nlp = spacy.blank(\"en\")\n",
    "ner = nlp.create_pipe(\"ner\")\n",
    "nlp.add_pipe(ner)\n",
    "ner.add_label(\"GADGET\")\n",
    "\n",
    "# Start the training\n",
    "nlp.begin_training()\n",
    "\n",
    "# Loop for 10 iterations\n",
    "for itn in range(10):\n",
    "    # Shuffle the training data\n",
    "    random.shuffle(TRAINING_DATA)\n",
    "    losses = {}\n",
    "\n",
    "    # Batch the examples and iterate over them\n",
    "    for batch in spacy.util.minibatch(TRAINING_DATA, size=2):\n",
    "        texts = [text for text, entities in batch]\n",
    "        annotations = [entities for text, entities in batch]\n",
    "\n",
    "        # Update the model\n",
    "        nlp.update(texts, annotations, losses=losses)\n",
    "        print(losses)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "{'ner': 11.999999642372131}\n",
    "\n",
    "{'ner': 21.978200435638428}\n",
    "\n",
    "{'ner': 32.035117387771606}\n",
    "\n",
    "{'ner': 9.415393471717834}\n",
    "\n",
    "{'ner': 14.99043345451355}\n",
    "\n",
    "{'ner': 19.80482855439186}\n",
    "\n",
    "{'ner': 2.9741987846791744}\n",
    "\n",
    "{'ner': 5.8216000609099865}\n",
    "\n",
    "{'ner': 7.243585231248289}\n",
    "\n",
    "{'ner': 2.2887884667434264}\n",
    "\n",
    "{'ner': 10.279852092295187}\n",
    "\n",
    "{'ner': 12.472829270933289}\n",
    "\n",
    "{'ner': 1.8872964698821306}\n",
    "\n",
    "{'ner': 4.788792780018412}\n",
    "\n",
    "{'ner': 7.156845846562646}\n",
    "\n",
    "{'ner': 1.9806696806917898}\n",
    "\n",
    "{'ner': 2.8503573195825993}\n",
    "\n",
    "{'ner': 4.763962420756343}\n",
    "\n",
    "{'ner': 1.3438352504745126}\n",
    "\n",
    "{'ner': 3.938637473517929}\n",
    "\n",
    "{'ner': 4.092377472894006}\n",
    "\n",
    "{'ner': 0.09106311114499022}\n",
    "\n",
    "{'ner': 2.694920648989994}\n",
    "\n",
    "{'ner': 2.7182347015319284}\n",
    "\n",
    "{'ner': 0.0014720811257120658}\n",
    "\n",
    "{'ner': 1.2311951145064413}\n",
    "\n",
    "{'ner': 1.2317418446089548}\n",
    "\n",
    "{'ner': 4.281543517836717e-06}\n",
    "\n",
    "{'ner': 0.0003685692108844618}\n",
    "\n",
    "{'ner': 2.4162862128263285}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 09. Best practices for training spaCy models\n",
    "### Problem 1: Models can \"forget\" things\n",
    "* Existing model can overfit on new data\n",
    "    * e.g.: if you only update it with `WEBSITE`, it can \"unlearn\" what a `PERSON` is\n",
    "* Also known as \"catastrophic forgetting\" problem\n",
    "\n",
    "### Solution 1: Mix in previously correct predictions\n",
    "* For example, if you're training `WEBSITE`, also include examples of `PERSON`\n",
    "* Run existing spaCy model over data and extract all other relevant entities\n",
    "\n",
    "__BAD__:\n",
    "\n",
    "`TRAINING_DATA = [\n",
    "    ('Reddit is a website', {'entities': [(0, 6, 'WEBSITE')]})\n",
    "]`\n",
    "\n",
    "__GOOD__:\n",
    "\n",
    "`TRAINING_DATA = [\n",
    "    ('Reddit is a website', {'entities': [(0, 6, 'WEBSITE')]}),\n",
    "    ('Obama is a person', {'entities': [(0, 5, 'PERSON')]})\n",
    "]`\n",
    "\n",
    "### Problem 2: Models can't learn everything\n",
    "* spaCy's models make predictions based on __local context__\n",
    "* Model can struggle to learn if decision is difficult to make based on context\n",
    "* Label scheme needs to be consistent and not too specific\n",
    "    * For example: `CLOTHING` is better than `ADULT_CLOTHING` and `CHILDRENS_CLOTHING`\n",
    "    \n",
    "### Solution 2: Plan your label scheme carefully\n",
    "* Pick categories that are reflected in local context\n",
    "* More generic is better than too specific\n",
    "* Use rules to go from generic labels to specific categories\n",
    "\n",
    "__BAD__:\n",
    "\n",
    "`LABELS = ['ADULT_SHOES', 'CHILDRENS_SHOES', 'BANDS_I_LIKE']`\n",
    "\n",
    "__GOOD__:\n",
    "\n",
    "`LABELS = ['CLOTHING', 'BAND']\n",
    "Let's practice!`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Good data vs. bad data\n",
    "Here’s an excerpt from a training set that labels the entity type TOURIST_DESTINATION in traveler reviews.\n",
    "\n",
    "`TRAINING_DATA = [\n",
    "    (\n",
    "        \"i went to amsterdem last year and the canals were beautiful\",\n",
    "        {\"entities\": [(10, 19, \"TOURIST_DESTINATION\")]},\n",
    "    ),\n",
    "    (\n",
    "        \"You should visit Paris once in your life, but the Eiffel Tower is kinda boring\",\n",
    "        {\"entities\": [(17, 22, \"TOURIST_DESTINATION\")]},\n",
    "    ),\n",
    "    (\"There's also a Paris in Arkansas, lol\", {\"entities\": []}),\n",
    "    (\n",
    "        \"Berlin is perfect for summer holiday: lots of parks, great nightlife, cheap beer!\",\n",
    "        {\"entities\": [(0, 6, \"TOURIST_DESTINATION\")]},\n",
    "    ),\n",
    "]`\n",
    "\n",
    "### Part 2\n",
    "* Rewrite the `TRAINING_DATA` to only use the label `GPE` (cities, states, countries) instead of `TOURIST_DESTINATION`.\n",
    "* Don’t forget to add tuples for the `GPE` entities that weren’t labeled in the old data.\n",
    "\n",
    "`TRAINING_DATA = [\n",
    "    (\n",
    "        \"i went to amsterdem last year and the canals were beautiful\",\n",
    "        {\"entities\": [(10, 19, \"TOURIST_DESTINATION\")]},\n",
    "    ),\n",
    "    (\n",
    "        \"You should visit Paris once in your life, but the Eiffel Tower is kinda boring\",\n",
    "        {\"entities\": [(17, 22, \"TOURIST_DESTINATION\")]},\n",
    "    ),\n",
    "    (\"There's also a Paris in Arkansas, lol\", {\"entities\": []}),\n",
    "    (\n",
    "        \"Berlin is perfect for summer holiday: lots of parks, great nightlife, cheap beer!\",\n",
    "        {\"entities\": [(0, 6, \"TOURIST_DESTINATION\")]},\n",
    "    ),\n",
    "]`\n",
    "\n",
    "### Answer\n",
    "\n",
    "`TRAINING_DATA = [\n",
    "    (\n",
    "        \"i went to amsterdem last year and the canals were beautiful\",\n",
    "        {\"entities\": [(10, 19, \"GPE\")]},\n",
    "    ),\n",
    "    (\n",
    "        \"You should visit Paris once in your life, but the Eiffel Tower is kinda boring\",\n",
    "        {\"entities\": [(17, 22, \"GPE\")]},\n",
    "    ),\n",
    "    (\n",
    "        \"There's also a Paris in Arkansas, lol\",\n",
    "        {\"entities\": [(15, 20, \"GPE\"), (24, 32, \"GPE\")]},\n",
    "    ),\n",
    "    (\n",
    "        \"Berlin is perfect for summer holiday: lots of parks, great nightlife, cheap beer!\",\n",
    "        {\"entities\": [(0, 6, \"GPE\")]},\n",
    "    ),\n",
    "]`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Training multiple labels\n",
    "Here’s a small sample of a dataset created to train a new entity type `WEBSITE`. The original dataset contains a few thousand sentences. In this exercise, you’ll be doing the labeling by hand. In real life, you probably want to automate this and use an annotation tool – for example, `Brat`, a popular open-source solution, or `Prodigy`, our own annotation tool that integrates with spaCy.\n",
    "\n",
    "### Part 1\n",
    "* Complete the entity offsets for the `WEBSITE` entities in the data. Feel free to use `len()` if you don’t want to count the characters.\n",
    "\n",
    "`TRAINING_DATA = [\n",
    "    (\n",
    "        \"Reddit partners with Patreon to help creators build communities\",\n",
    "        {\"entities\": [(____, ____, \"WEBSITE\"), (____, ____, \"WEBSITE\")]},\n",
    "    ),\n",
    "    (\"PewDiePie smashes YouTube record\", {\"entities\": [(____, ____, \"WEBSITE\")]}),\n",
    "    (\n",
    "        \"Reddit founder Alexis Ohanian gave away two Metallica tickets to fans\",\n",
    "        {\"entities\": [(____, ___, \"WEBSITE\")]},\n",
    "    ),\n",
    "    # And so on...`\n",
    "    \n",
    "### Answer\n",
    "`TRAINING_DATA = [\n",
    "    (\n",
    "        \"Reddit partners with Patreon to help creators build communities\",\n",
    "        {\"entities\": [(0, 6, \"WEBSITE\"), (21, 28, \"WEBSITE\")]},\n",
    "    ),\n",
    "    (\"PewDiePie smashes YouTube record\", {\"entities\": [(18, 25, \"WEBSITE\")]}),\n",
    "    (\n",
    "        \"Reddit founder Alexis Ohanian gave away two Metallica tickets to fans\",\n",
    "        {\"entities\": [(0, 6, \"WEBSITE\")]},\n",
    "    ),\n",
    "    # And so on...\n",
    "]`\n",
    "\n",
    "### Part 3\n",
    "Update the training data to include annotations for the `PERSON` entities “PewDiePie” and “Alexis Ohanian”.\n",
    "\n",
    "`TRAINING_DATA = [\n",
    "    (\n",
    "        \"Reddit partners with Patreon to help creators build communities\",\n",
    "        {\"entities\": [(0, 6, \"WEBSITE\"), (21, 28, \"WEBSITE\")]},\n",
    "    ),\n",
    "    (\"PewDiePie smashes YouTube record\", {\"entities\": [____, (18, 25, \"WEBSITE\")]}),\n",
    "    (\n",
    "        \"Reddit founder Alexis Ohanian gave away two Metallica tickets to fans\",\n",
    "        {\"entities\": [(0, 6, \"WEBSITE\"), ____]},\n",
    "    ),\n",
    "    # And so on...`\n",
    "    \n",
    "### Answer\n",
    "\n",
    "`TRAINING_DATA = [\n",
    "    (\n",
    "        \"Reddit partners with Patreon to help creators build communities\",\n",
    "        {\"entities\": [(0, 6, \"WEBSITE\"), (21, 28, \"WEBSITE\")]},\n",
    "    ),\n",
    "    (\n",
    "        \"PewDiePie smashes YouTube record\",\n",
    "        {\"entities\": [(0, 9, \"PERSON\"), (18, 25, \"WEBSITE\")]},\n",
    "    ),\n",
    "    (\n",
    "        \"Reddit founder Alexis Ohanian gave away two Metallica tickets to fans\",\n",
    "        {\"entities\": [(0, 6, \"WEBSITE\"), (15, 29, \"PERSON\")]},\n",
    "    ),\n",
    "    \n",
    "]`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
